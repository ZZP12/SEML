using ArgParse
# using Dates


function parse_commandline()
  settings_object = ArgParseSettings()
  @add_arg_table settings_object begin
    "-o"
      help = "Directory where the Julia model files will be written."
      arg_type = AbstractString
      default = "./autogeneratedmodel"

    "-m"
      help = "Path to the model file written in the NML format."
      arg_type = AbstractString
      required = true

    "-s"
      help = "Host type: bacteria or mammalian?"
      arg_type = Symbol
      default = :bacteria

    "-r"
      help = "Modeling framework: FBA or Kinetics"
      arg_type = AbstractString
      default = "Kinetics"

    "-l"
      help = "Language: julia or python3 or python2 or python or matlab?"
      arg_type = AbstractString
      default = "julia"
  end
  return parse_args(settings_object)
end

parsed_args = parse_commandline()

path_head = dirname(Base.source_path())

# Error report: line number --> error message
error_report = Dict{Int64, Array}()

include(joinpath(path_head, "preprocessor3.jl"))
println("\n---------loading sentences------------")
# inputSentencesArray = Array of pairs of (line, line #)
inputSentencesArray = getRidOfNewlineHashcommentBlankline(parsed_args["m"])
println("\n---------tokenizing sentences------------")
# tokenizedInputSentencesArray = Array of pairs of (tokens, line #)
tokenizedInputSentencesArray = sentenceTokenization2(inputSentencesArray)
println("\n---------normalizing and tagging------------")
reservedWordsPath = joinpath(path_head, "reservedWords.jl")
# taggedSentencesArray = Array of pairs of (taggedTokens, line #)
taggedSentencesArray = tokenClassification2(tokenizedInputSentencesArray, reservedWordsPath)
# reshape for output observation
printTagSen = [["$(y[1])/$(y[2])" for y in x] for (x, id) in taggedSentencesArray]
# println(typeof(printTagSen))
foreach(println, [join(ts, "  ") for ts in printTagSen])

println("\n------------information extraction------------")
include(joinpath(path_head, "biosymDecoder.jl"))
include(reservedWordsPath)
# BioSymVerbInfo = Array of pairs of ((senType, line#), BioSym, paraSet)
BioSymVerbInfo = extractBioSymVerbInformation(taggedSentencesArray,
                 reservedWords["SentenceType"], error_report)
# println("print by foreach:")
foreach(println, BioSymVerbInfo)
println("\n------------decoding bio symbols------------")
decodingBioSymGroups(BioSymVerbInfo, error_report)
# BioSymVerbInfo = Array of pairs of ((senType, line#), BioSym, paraSet)
println("\n------------decoding bio symbols results------------")
printArrayOfTripleToken(BioSymVerbInfo, AbstractString)
println("\n------------type conversion dictionary---------------")
typeConversionDict = setUpSymbolConvertionDict(BioSymVerbInfo, error_report)
foreach(println, typeConversionDict)

println("\n------------replace each biosymbol string with generalBioSym
        composite-----------------")
# newVerbBiosymInfo = Array of ((senType, line#), BioSym, paraSet)
newVerbBiosymInfo = replaceEachBioSymWithGeneralBioSym(BioSymVerbInfo,
                    typeConversionDict, error_report)
printArrayOfTripleToken(newVerbBiosymInfo, generalBioSym)

println("\n------------right before IR generation-----------------")
include(joinpath(path_head, "semanticChecking.jl"))
# preIR_DS = array of tuple ((verb, line #), reactant, product, catalyst)
preIR_DS = semanticCheckingForEachTriplet(newVerbBiosymInfo,
           typeConversionDict, error_report)
printArrayOfTripleToken(preIR_DS, generalBioSym)

#######################
# TO HERE, no more error report after this line
#######################
# print errors
if length(error_report) != 0
  println("\n\n\n#################### Error report: Useful tips ########")
  for lineNum in sort(collect(keys(error_report)))
    println("in line $lineNum -->", )
    for err in error_report[lineNum]
      println("\t$(err)")
    end
  end
  exit()
end



println("\n------------model generation preparation------------------")
include(joinpath(path_head, "modelGeneration.jl"))
sys2userDict = Dict()
for (key, val) in typeConversionDict
  sys2userDict[val] = key
end
# reform to match with NML_V1
# preIR_DS = array of tuple ((verb, line #), reactant, product, catalyst)
(rnx, txtl, rnx_set, mRNA_set, m_protein_set) = preparing_rnxList_txtlDict(
  preIR_DS, sys2userDict, typeConversionDict)

println("------------rnx list----")
foreach(x->println(x.rnxName), rnx)
# Sorting, insert "BIOMASS" here
(sorted_rnx_species_array, rnx_species2index_dict, rnx_index2species_dict,
 sorted_all_species_array, all_species2index_dict, all_index2species_dict, extra_species_num) =
  sorting_species_list(rnx_set, mRNA_set, m_protein_set, sys2userDict)
println("------------")
foreach(println, sorted_rnx_species_array)
println("------------")
foreach(println, sorted_all_species_array)

# write program to disk
output_file = Dict{String, String}()  # put all files in this dict
# stoichiometric_matrix
stoichiometric_matrix_buffer = build_stoichiometric_matrix_buffer(rnx, rnx_species2index_dict)
output_file["stoichiometry.dat"] = stoichiometric_matrix_buffer

println("\n------------Modeling Framework-----")
# DISTRIBUTION file mapping
DISTRIBUTION = Dict(
    "julia"   => ("distribution/dis.jl", "JuliaStrategy.jl"),
    "python2" => ("distribution/dis.py2", "Python2Strategy.jl"),
    "python3" => ("distribution/dis.py3", "Python3Strategy.jl"),
    "matlab"  => ("distribution/dis.m", "MATLABStrategy.jl")
)
target_lang = parsed_args["l"]
model_frame = parsed_args["r"]
if target_lang == "julia"
  include(joinpath(path_head, DISTRIBUTION["julia"][2]))
  file_suffix = ".jl"
  dis_file_folder = DISTRIBUTION["julia"][1]
elseif ((target_lang == "python3") || (target_lang == "python"))
  include(joinpath(path_head, DISTRIBUTION["python3"][2]))
  file_suffix = ".py"
  dis_file_folder = DISTRIBUTION["python3"][1]
elseif target_lang == "python2"
  include(joinpath(path_head, DISTRIBUTION["python2"][2]))
  file_suffix = ".py"
  dis_file_folder = DISTRIBUTION["python2"][1]
elseif  target_lang == "matlab"
  include(joinpath(path_head, DISTRIBUTION["matlab"][2]))
  file_suffix = ".m"
  dis_file_folder = DISTRIBUTION["matlab"][1]
else
  println("Generate model in julia as default")
  include(joinpath(path_head, DISTRIBUTION["julia"][2]))
  target_lang = "julia"
  file_suffix = ".jl"
  dis_file_folder = DISTRIBUTION["julia"][1]
end
# Generate FBA model
if model_frame == "FBA"
   if target_lang == "matlab"
      data_dictionary_buffer, maximize_product_buffer = generate_FBA_data_dictionary(
             rnx, sorted_rnx_species_array, extra_species_num)
      output_file["DataDictionary" * file_suffix] = data_dictionary_buffer
      output_file["maximizeProductDictionary" * file_suffix] = maximize_product_buffer
  else
      data_dictionary_buffer = generate_FBA_data_dictionary(rnx,
                                 sorted_rnx_species_array, extra_species_num)
      output_file["DataDictionary" * file_suffix] = data_dictionary_buffer
  end
  # write & copy
  output_dir = parsed_args["o"]
  if !(isdir(output_dir))
    mkpath(output_dir)
  end
  for (file_name, file) in output_file
    write(joinpath(output_dir, file_name), file)
  end
  for file in readdir(joinpath(path_head, dis_file_folder))
    if contains(file, file_suffix)
        println(file)
        f = joinpath(path_head, dis_file_folder, file)
        cp(f, joinpath(output_dir, file); remove_destination=true)
    end
  end
  cp(parsed_args["m"], joinpath(output_dir, basename(parsed_args["m"])); remove_destination=true)
# Generate Kinetic model
else
  (kinetics_buffer, Monod_const, W_array, disassociation_const) =
    build_kinetics_buffer(all_species2index_dict, rnx, txtl, sys2userDict)
  output_file["Kinetics" * file_suffix] = kinetics_buffer

  host_type = parsed_args["s"]
  data_dictionary_buffer = build_data_dictionary_buffer(host_type, sorted_all_species_array,
    all_species2index_dict,
    sorted_rnx_species_array, rnx, txtl, Monod_const, W_array, disassociation_const,
    collect(mRNA_set), collect(m_protein_set))
  output_file["DataDictionary" * file_suffix] = data_dictionary_buffer

  ODE_simulation_buffer = build_simulation_buffer(extra_species_num)
  output_file["Balances" * file_suffix] = ODE_simulation_buffer

  solveODEBalances_buffer = build_solveODEBalances_buffer(sorted_all_species_array,
    all_species2index_dict, collect(mRNA_set), collect(m_protein_set))
  output_file["SolveBalances" * file_suffix] = solveODEBalances_buffer

  output_dir = parsed_args["o"]
  if !(isdir(output_dir))
    mkpath(output_dir)
  end
  if target_lang == "julia"
    cp(joinpath(path_head, "include.jl"), joinpath(output_dir, "include.jl"); remove_destination=true)
  end
  cp(parsed_args["m"], joinpath(output_dir, basename(parsed_args["m"])); remove_destination=true)
  for (file_name, file) in output_file
    write(joinpath(output_dir, file_name), file)
  end
end
